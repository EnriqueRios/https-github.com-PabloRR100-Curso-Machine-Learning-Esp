{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de Decisión\n",
    "---\n",
    "\n",
    "- 1 - Introducción\n",
    "- 2 - Árboles de Regresión\n",
    "- 3 - Árboles de Clasificiación\n",
    "- 4 - Algorimot ID3\n",
    "- 5 - Podado (prunning) y Validación Cruzada\n",
    "- 6 - Algorimto C43\n",
    "\n",
    "## Introducción\n",
    "---\n",
    "Los métodos basados en árboles *tree-based* recurren a la **estratificación** o **segmentacion** del espacio de los predictores. Resulta que, como las reglas que se han utilizado para describir el árbol se pueden representar en forma de árbol, estos métodos reciben el nombre de árboles de decisión.\n",
    "\n",
    "<img src='https://users.dcc.uchile.cl/~bebustos/apuntes/cc30a/Ordenacion/arbol-decision.gif'>\n",
    "\n",
    "Al primer nodo, arriba del todo, se le conoce como nodo raíz (**root node**). A los nodos finales, las salidas del modelo en función de las entradas, se les conoce como nodos hojas (**leaf node**). Cada valor de entra Xi genera una división, que en la analogía con el árbol será coger una rama u otra, como se ve en la imagen.\n",
    "\n",
    "Los árboles de decisión pueden ser utilizados tanto para regresión como para clasificiación. Empezaremos explicando los de regresión y después explicaremos los de regresión.\n",
    "\n",
    "En este [link](http://www.cs.us.es/~fsancho/?e=104) hay una muy buena introducción en español al algoritomo de Árboles de Decisión ID3.\n",
    "\n",
    "#### ¿Cúando conviene utilizar árboles de decisión?\n",
    "En general, los árboles de decisión son **buenos clasificadores**. Los criterios más comunes para determinar que necesitamos un árbol de decisión son:  \n",
    "\n",
    "- Las instancias se representan por un conjunto de atributos con valores nominales (aunque también pueden ser numéricos con el algoritmo adecuado)\n",
    "- La función objetivo tiene valores de salida discretos (como SI / NO)\n",
    "- Se requiren descripciones disyuntivas. Las ramas de los árboles son sucesiones de Yes y Oes a cada pregunta en un nodo\n",
    "- Los datos de entrenamiento contienen errores o valores de atributos desconocidos. Los árboles son robustos frente a errores.\n",
    "\n",
    "#### Ventajas de los Árboles de Decisión\n",
    "- Faciles de comprender y traducir a reglas en lenguaje natural\n",
    "- Pueden trabajar con datos numéricos y nominales\n",
    "- Pueden trabajar con datos multi-dimensionales\n",
    "- No requieren conocimiento en un dominio ni establecer parámetros que necesiten determinarse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árboles de Regresión\n",
    "---\n",
    "La técnica para construir árboles de regresión y clasificación son similares. Empezaremos explicando árboles de regresión porque los criterios seguidos para construir las ramas del árbol son más sencillas, y a parte las imágenes que mostraremos permiten ver de forma muy sencilla que está haciendo el árbol en el espacio de búsquda de soluciones.  \n",
    "\n",
    "El proceso de construir un árbol de regresión tiene por tanto 2 sencillos pasos:  \n",
    "\n",
    "* Dividir el conjunto de variables, para simplificar iremos con X1 y X2, en J distintas, y NO superpuestas, regiones. \n",
    "* Para cada futura predición cuyos valores de entrada caigan en la región Rj, su valor será simplemente el valor medio de las observaciones que ya haya en esa región.  \n",
    "\n",
    "Y entonces, **¿cómo se eligen las regiones?**  \n",
    "El objetivo es encontrar la regiones R1, ..., Rj que minimicen el **RSS** (Residual Sum of Squared), que no es más que la suma de los errores al cuadrado con la que estamos jugando siempre.\n",
    "<img src='./Imagenes/trees/RSS.png'>\n",
    "Está claro que **computacionalmente no es posible** calcular todas las combinaciones de particiones posibles. Entonces, ¿qué podemos hacer?  \n",
    "\n",
    "Realizamos los que se llama una búsqueda *greedy top-down recursive binary splitting*. Literalmente sería, vamos de arriba a abajo diviendo las regiones en 2, sin mirar a atrás; es decir, cogiendo siempre la mejor separación que tenemos ahora, sin pararnos a pensar que quizas en el futuro otra partición hubiera sido más interesante.  \n",
    "\n",
    "En el árbol, cada una de estas separaciones que decimos corresponde con cada una de las 2 ramas que salen de 1 nodo.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividiendo el espacio de los predictores\n",
    "Entonces, siendo j una de las variables independientes, X1 o X2 (luego j = 1 o j = 2); tenemos que encontrar un punto de separación s de tal manera que minimicemos el RSS total de cada uno de los 2 espacios:\n",
    "<img src='./Imagenes/trees/RSS_total.png'>\n",
    "\n",
    "Este proceso se repite hasta que se cumple algún **criterio de parada**. Existen diferentes motivos por los que querríamos parar el algoritmo, uno de ellos por ejemplo es que en cada región hubiera un máximo de puntos (observaciones). La siguiente figura muestra el resultado de una división hasta 5 regiones para dos variables (ignorar la de arriba a la izquierda):\n",
    "<img src='./Imagenes/trees/reg_tree.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árboles de Clasificación\n",
    "---\n",
    "Si hacemos la analogía con los árboles de regresión, hemos de tener en cuenta que en los problemas de clasificación se analizan variables cualitativas.  \n",
    "\n",
    "Esto implica que no podremos tomar la media de los valores de una región para predecir, y la alternativa natural es coger la moda, es decir, la clase que más se repite dentro de todas las clases que haya en la región.  \n",
    "\n",
    "De igual forma, no se podrá calcular el RSS como criterio para la división de las regiones. La alternativa será el ratio de error de clasficación, es decir, la fracción de clases de la región que no sean iguales a la más común:  \n",
    "<img src='./Imagenes/trees/error.png'>\n",
    "Donde p_m_k es la **p**roporción de observaciones de la clase **k** que caen en la región **m**.  \n",
    "\n",
    "Sin embargo, en la práctica este error no es lo suficientemente sensible para realizar buenas divisiones, y se utilizan otros índices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropía de Información\n",
    "<img src='./Imagenes/trees/entropy.png'>\n",
    "Mide la heterogeneidad de un conjunto. \n",
    "p_i es la **p**roporción de observaciones de la clase **i** entre las *n* clases totales.\n",
    "* E valdrá 0 si todas las observaciones de un nodo (región Rj) son de la misma clase. \n",
    "* E valdrá 1 si todas las observaciones del nodo corresponde cada una a una clase distinta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gananacia de Información\n",
    "Utiliza la entropía para media la efectividad de un atributo para clasificar ejemplos.  \n",
    "Mide la reducción de entropía cuando se distribuyen los E ejemplos de acuerdo a un atributo A con Va valores posibles.\n",
    "<img src='./Imagenes/trees/ganancia.png'>\n",
    "\n",
    "#### Índice de Gini\n",
    "<img src='./Imagenes/trees/gini.png'>\n",
    "Mide la varianza total a través de todas las K clases. Suele decirse que mide la **pureza** de un nodo. \n",
    "* G valdrá 0 si todas las observaciones de un nodo (región Rj) son de la misma clase. \n",
    "* G valdrá 1 si todas las observaciones del nodo corresponde cada una a una clase distinta.\n",
    "\n",
    "\n",
    "#### Proporción de ganancia\n",
    "Cuando se manejan atributos con muchos valores, la proporción de ganancia puede dar mejores resultados que la ganancia de información, que favorece a los atributos con mayor número de valores.  \n",
    "\n",
    "En el caso extremo de 1 atributo con 1 valor diferente en cada emeplo, la medida de ganancia determinará que este atributo sería un buen clasificador, cuando en realidad no lo será para nuevas instancias.  \n",
    "\n",
    "La proporción de ganancia compensa el hecho de que un atributo pueda tener muchos valores, dividiendo por la **información de la división** \n",
    "<img src='./Imagenes/trees/inf_division.png'>\n",
    "\n",
    "De esta manera se consigue pensalizar aquellos atributos con muchos valores que se distribuyen muy uniformemente entre ejemplos: \n",
    "<img src='./Imagenes/trees/prop_ganancia.png'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo ID3 \n",
    "---\n",
    "El algoritmo ID3 construye los árboles de decisión utilizando como índice que mejor distribuye los ejemplos la **ganancia de información**.  \n",
    "\n",
    "ID3 tiene la ventaja de que trabaja en un espacio de hipótesis completo, por lo tanto con la seguridad de que la solución está en el espacio de búsqueda. Por otro lado, ID3 no contempla varias posibles soluciones al mismo tiempo, con lo que corre el riesgo de construir un árbol final que no es la mejor solución.  \n",
    "\n",
    "ID3 utiliza todos los ejemplos en cada decisión. Esto supone mucha **carga computacional**, pero al mismo tiempo, le otorga **robustez** ante errores.  \n",
    "\n",
    "Como analogía a lo que vimos en el árbol de regresión, ID3 es una búsqueda greedy, sin mirar hacia atrás. Situará los atributos que proporcionan mayor ganancia más cerca del nodo raíz, **sin garantizar encontrar el árbol más corto**, que haría una búsqueda más exhaustiva que simplemente ordenar por ganancia de información.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning - Poda\n",
    "---\n",
    "Aunque parezca contradictorio, **predecir demasiado bien no es siempre adeacuado**. Esto es así porque, los datos de entrenamiento pueden contener ruido o simplemente pueden no ser una muestra suficientemente representativa de los datos como para generalizar.  \n",
    "\n",
    "Si construímos en base a esta mala muestra del conjunto de entrenamiento un árbol que sea muy preciso, estaremos cayendo en el problema de **overfitting** y las predicciones sobre observaciones futuras serán muy pobres.  \n",
    "\n",
    "El método de poda o prunning, como su nombre indica, consiste en la *'poda'* o eliminación de nodos y ramas del árbol, y se puede efectuar de 2 diferentes métodos:  \n",
    "\n",
    "##### ¿Cómo se puede saber cuál es el tamaño del árbol adecuado?\n",
    "Una de las técnicas más comunes es la validación cruzada (**cross-validation (CV)**). Esta técnica permite estimar cómo se ajustará el modelo que creamos con los datos de entrenamiento ante futuros nuevos datos que el modelo no ha visto durante su construcción, el conjunto de validación.  \n",
    "\n",
    "La extensión más habitual entre las posibles permutación de CV es el conocido **k-Fold CV**, el cual dividie el conjunto de datos de entrenamiento en k grupos. De estos k grupos, 1 actuará como conjunto de validación, y el modelo se construirá con los otros (k-1) grupos. Esto se repite para todas las combinaciones de los k para que cada vez cada uno sea el conjunto de validación, y se promedia el resultado que se obtuvo en las k validaciones. \n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/f/f2/K-fold_cross_validation.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorimto C4.5\n",
    "---\n",
    "El algoritmo C4.5 es una evolución del algoritmo ID3.  \n",
    "\n",
    "Mientras que el algoritmo ID3 se aplica a atributos discretos, **C4.5 se puede aplicar a atributos con valores tanto discretos como continuos**. En el caso de trabajar con valores contínuos, la respuesta a la pregunta que efectúa el nodo para separar no puede ser binaria, como trabajábamos antes. Es decir, para variables categóricas podíamos preguntar ¿soleado? y la respuesta sería si, no, un poco... y tantas clases para esa pregunta como podamos imaginar. Ahora, con valores continuos, la pregunta (**el antecedente**) para hacer la división en el nodo podría ser ¿temperatura? y la respuesta es un único valor numérico. Es por ello, que se determina un **umbral** para hacer la divisiones. Por ejemplo, una rama será temperatura < 20ºC, mientras que la otra será temperatura >= 20ºC.  \n",
    "\n",
    "El algorimto C4.5 también utiliza la **proporción de ganancia** como método de selección de atributos.\n",
    "\n",
    "C.4 realiza poda tras generación, **pospoda** para mejorar el poder de clasificación del modelo. **¿Cómo estimar la precisión (o el poder) de clasificación del árbol??**  \n",
    "Aunque estadísiticamente parezca pobre, **C4.5 utiliza valores del conjunto de entrenamiento para la validación**, con sorprendentes buenos resultados.  \n",
    "Tras la poda, C4.5 asigna a un nodo la clase mayoritaria de ese nodo. Como habrá otras clases que no son la mayoritaria, la tasa de errores será la proporción de esas clases que no son la mayoritaria, Te = e / N. Donde e son el número de ejemplos de otras clases que no son la mayoritaria y N el número total de ejemplos.  \n",
    "Esta estimación es realizada sobre el conjunto de entrenamiento y, por tanto, es muy optimista y está sesgada. Para compensar el hecho de emplear los propios datos del conjunto de entrenamiento, realiza una **poda pesimista**, ajustando la tasa de error con una penalización al valor obtenido.  \n",
    "\n",
    "Es decir, para un determinado nivel de confianza, toma el nivel más desfavorable del intervalo de confianza como medida del error. C4.5 utiliza como valor por defecto una confianza de c=0.25 y, para obtener la tasa real del error a patir de la observada, obtiene el límite superior del intervalo de confianza, a partir de:\n",
    "<img src='./Imagenes/trees/poda_pesimista.png'>\n",
    "\n",
    "Tomando el límite superior del intervalo se obtiene:\n",
    "<img src='./Imagenes/trees/limite_superior.png'>\n",
    "Siendo z=0.68 para un valor de c=0.25 -- valores en las tablas --\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
