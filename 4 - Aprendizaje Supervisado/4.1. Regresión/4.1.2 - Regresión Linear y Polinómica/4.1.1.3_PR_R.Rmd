---
title: "Regresión Polinómica"
output: html_document
---

#Regresión Polinómica (R)
---
La regresión linear utiliza el método de mínimos cuadrados para encontrar la recta que resulta en la menor suma de errores al cuadrado (RMSE: Root Mean Square Error).
La palabra simple se refiere a que la variable respuesta solo depende de 1 variable independiente: Y = f(X)


Escenario del problema
---

<img src='../Imagenes/entrevista.png' style='width:8%;height:8%;float:left;margin-right:20px'>
Vamos a contratar un nuevo empleado. Nos ha dicho que en su anterior empresa fue Manager Regional durante 2 años y que cobraba 170.000€ al año. Queremos determinar hasta que punto nos dice la verdad para poder negociar con él el salario que queremos ofrecerle en su nuevo puesto.  
¡Vamos a ello!

<br>
<br>

```{r 1. Importar librerías}
# 1. Importar librerías
library(caTools)
library(ggplot2)
```

```{r 2. Importar datos}
# 2. Importar datos
datos <- read.csv('../Datos/4.1.Salarios2.csv')
datos <- datos[2:3] # Eliminamos la columna del título del puesto y nos quedamos con el nivel
head(datos, 10)
ggplot() + 
  geom_line(aes(x=datos$Nivel, y=datos$Salario), colour='blue') +
  ggtitle('Relación entre Salario y Nivel de puesto de trabajo') +
  xlab('Nivel') +
  ylab('Salario')
```
Vemos como NO existe una tendencia lineal

```{r # 3. Histograma de Salario}
# 3. Histograma de Salario
ggplot(data=datos, aes(datos$Salario)) +
  geom_histogram(bins=10,
                 col='black',
                 fill='blue',
                 alpha=0.2) +
  xlab('Salario') + 
  ylab('Frecuencia') +
  geom_vline(xintercept = mean(datos$Salario), col='brown')
```


```{r # 3. Separar en Entrenamiento y Validación}
# 3. Separar en Entrenamiento y Validación
```
**Recordatorio:** no hacemos división de conjuntos porque tenemos muy pocos datos y nuestra intención es hacer una predicción lo más precisa posible.

```{r # 4. Construir el Modelo}
# 4. Construir el Modelo
# 4.1 Tenemos que construir una nueva variable para cada grado de polinomio que queramos. Si por ejemplo, queremos un polinomio de grado 4: (Y = C + aX + bX^2 + cX^3 + dX4)
datos$Nivel2 = datos$Nivel^2
datos$Nivel3 = datos$Nivel^3
datos$Nivel4 = datos$Nivel^4
head(datos, 5)

regresor_lineal <- lm(formula = Salario ~ Nivel, data = datos)
regresor_polino <- lm(formula = Salario ~ ., data = datos)

```

```{r 5. Hacer las prediciones para el conjunto de Validación}
# 5. Hacer las prediciones para el conjunto de Validación
y_fit_lineal <- predict(regresor_lineal, newdata = datos)
y_fit_polino <- predict(regresor_polino, newdata = datos)
```
No estamos prediciendo en este ejemplo, sino determinando los parámetros para que el modelo se **ajuste** lo mejor posible a los datos del conjunto de entrenamiento (que constitutye todos los datos)

```{r 6. Echemos un vistazo a la pinta que tienen las predicciones}
# 6. Echemos un vistazo a la pinta que tienen las predicciones
ggplot() +
  geom_point(aes(datos$Nivel, datos$Salario), colour='red') +
  geom_line(aes(datos$Nivel, y_fit_lineal), colour='purple') +
  geom_line(aes(datos$Nivel, y_fit_polino), colour='blue') +
  ggtitle('Regresión Polinómica') +
  xlab('Años de experiencia') +
  ylab('Salario')
```

```{r 7. Calcular el error}
# 7. Calcular el error
library(Metrics)
y_real <- datos$Salario
RMSE_lineal <- rmse(y_real, y_fit_lineal)
RMSE_polino <- rmse(y_real, y_fit_polino)
cat('El error lineal es de ', RMSE_lineal, ' mientras que el error del polinómico es de ', RMSE_lineal)
avg <- mean(datos$Salario)
err <- RMSE_polino * 100 / avg
cat('Teniendo en cuenta un salario medio de ', avg, ' el error es del ', err, '%')
```
