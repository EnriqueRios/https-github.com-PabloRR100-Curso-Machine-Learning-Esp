points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(e1071)         # Nos va a permitir general el modelo de Bayes
datos <- read.csv('../Datos/4.2.Compras.csv')
datos <- datos[3:5] # Eliminamos la columna del sexo
datos$Compra <- factor(datos$Compra,
levels = c(0, 1),
labels = c(0, 1))
set.seed(123)
split <- sample.split(datos$Compra, SplitRatio = 0.75)
train <- subset(datos, split==TRUE)
test  <- subset(datos, split==FALSE)
train[-3] <- scale(train[-3])
test[-3]  <- scale(test[-3])
y_pred <- e1071::naiveBayes(formula = Salario ~ .,
data = train)
y_pred <- predict(bayesiano, newdata = test)
y_pred <- predict(bayesiano, newdata = test)
bayesiano <- e1071::naiveBayes(formula = Salario ~ .,
data = train)
y_pred <- predict(bayesiano, newdata = test)
cm = table(test[ , 3], y_pred)
bayesiano <- e1071::naiveBayes(x = train[-3],
y = train$Compra)
y_pred <- predict(bayesiano, newdata = test)
cm = table(test[ , 3], y_pred)
cm
# 8. Echemos un vistazo a la pinta que tienen las predicciones
# 8.1. Conjunto de entrenamiento
set = train
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_grid = predict(bayesiano, newdata = grid_set)
plot(set[, -3],
main = 'Bayesiano (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# 8.2. Conjunto de validación
set = test
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_grid = predict(bayesiano, newdata = grid_set)plot(set[, -3],
# 1. Importar librerías
library(caTools)
library(ggplot2)
library(e1071)         # Nos va a permitir general el modelo de Bayes
library(ElemStatLearn) # Nos va a permitir dibujar las clasificaciones
# 2. Importar datos
datos <- read.csv('../Datos/4.2.Compras.csv')
datos <- datos[3:5] # Eliminamos la columna del sexo
# 2.1. Codificiar la respuesta como categórica (factores en R)
datos$Compra <- factor(datos$Compra,
levels = c(0, 1),
labels = c(0, 1))
head(datos, 5)
# 3. Separar en Entrenamiento y Validación
set.seed(123)
split <- sample.split(datos$Compra, SplitRatio = 0.75)
train <- subset(datos, split==TRUE)
test  <- subset(datos, split==FALSE)
dim(train)/dim(test)
# 4. Hacer las prediciones para el conjunto de Validación
train[-3] <- scale(train[-3])
test[-3]  <- scale(test[-3])
# 5. Construir el Modelo y Predecir (kNN lo hace todo junto)
bayesiano <- e1071::naiveBayes(x = train[-3],
y = train$Compra)
# 6. Construir el Modelo y Predecir (kNN lo hace todo junto)
y_pred <- predict(bayesiano, newdata = test)
# 7. Hacer la matriz de confusión
cm = table(test[ , 3], y_pred)
cm
# 8. Echemos un vistazo a la pinta que tienen las predicciones
# 8.1. Conjunto de entrenamiento
set = train
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_grid = predict(bayesiano, newdata = grid_set)
plot(set[, -3],
main = 'Bayesiano (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# 8.2. Conjunto de validación
set = test
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_grid = predict(bayesiano, newdata = grid_set)
plot(set[, -3],
main = 'Bayesiano (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(rpart)         # Nos va a pertmit crear el arbol
datos <- read.csv('../Datos/4.2.Compras.csv')
datos <- datos[3:5] # Eliminamos la columna del sexo
set.seed(123)
split <- sample.split(datos$Compra, SplitRatio = 0.75)
train <- subset(datos, split==TRUE)
test  <- subset(datos, split==FALSE)
train[-3] <- scale(train[-3])
test[-3]  <- scale(test[-3])
arbol <- rpart::rpart(formula = Compra ~ .,
data = train,
control = rpart.control(minsplit=1))
y_pred <- predict(arbol, newdata = test)
cm = table(test[, 3], y_pred > 0.5)
cm
arbol <- rpart::rpart(formula = Compra ~ .,
data = train,
type = 'class',
control = rpart.control(minsplit=1))
y_pred <- predict(arbol, newdata = test, type = 'class')
y_pred <- predict(arbol, newdata = test[-3], type = 'class')
train <- subset(datos, split==TRUE)
test  <- subset(datos, split==FALSE)
train[-3] <- scale(train[-3])
test[-3]  <- scale(test[-3])
arbol <- rpart::rpart(formula = Compra ~ .,
data = train,
control = rpart.control(minsplit=1))
y_pred <- predict(arbol, newdata = test[-3], type = 'class')
datos$Compra <- factor(datos$Compra,
levels = c(0, 1),
labels = c(0, 1))
set.seed(123)
split <- sample.split(datos$Compra, SplitRatio = 0.75)
train <- subset(datos, split==TRUE)
test  <- subset(datos, split==FALSE)
train[-3] <- scale(train[-3])
test[-3]  <- scale(test[-3])
arbol <- rpart::rpart(formula = Compra ~ .,
data = train,
control = rpart.control(minsplit=1))
y_pred <- predict(arbol, newdata = test[-3], type = 'class')
y_pred <- predict(arbol, newdata = test[-3], type = 'class')
cm = table(test[, 3], y_pred > 0.5)
cm = table(test[, 3], y_pred)
cm
# 8. Echemos un vistazo a la pinta que tienen las predicciones
# 8.1. Conjunto de entrenamiento
set = train
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_pred <- predict(arbol, newdata = grid_set, type = 'class')
plot(set[, -3],
main = 'Regresión Logística (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# 8.2. Conjunto de validación
set = test
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_pred <- predict(arbol, newdata = grid_set, type = 'class')
plot(set[, -3],
main = 'Regresión Logística (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# 1. Importar librerías
library(caTools)
library(ggplot2)
library(rpart)         # Nos va a pertmit crear el arbol
library(ElemStatLearn) # Nos va a permitir dibujar las clasificaciones
# 2. Importar datos
datos <- read.csv('../Datos/4.2.Compras.csv')
datos <- datos[3:5] # Eliminamos la columna del sexo
datos$Compra <- factor(datos$Compra,
levels = c(0, 1),
labels = c(0, 1))
head(datos, 5)
# 3. Separar en Entrenamiento y Validación
set.seed(123)
split <- sample.split(datos$Compra, SplitRatio = 0.75)
train <- subset(datos, split==TRUE)
test  <- subset(datos, split==FALSE)
dim(train)/dim(test)
# 4. Hacer las prediciones para el conjunto de Validación
train[-3] <- scale(train[-3])
test[-3]  <- scale(test[-3])
# 5. Construir el Modelo
arbol <- rpart::rpart(formula = Compra ~ .,
data = train,
control = rpart.control(minsplit=1))
# 6. Hacer las prediciones para el conjunto de Validación
y_pred <- predict(arbol, newdata = test[-3], type = 'class')
# 7. Hacer la matriz de confusión
cm = table(test[, 3], y_pred)
cm
# 8. Echemos un vistazo a la pinta que tienen las predicciones
# 8.1. Conjunto de entrenamiento
set = train
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_pred <- predict(arbol, newdata = grid_set, type = 'class')
plot(set[, -3],
main = 'Regresión Logística (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
# 1. Importar librerías
library(caTools)
library(ggplot2)
library(rpart)         # Nos va a pertmit crear el arbol
library(ElemStatLearn) # Nos va a permitir dibujar las clasificaciones
# 2. Importar datos
datos <- read.csv('../Datos/4.2.Compras.csv')
datos <- datos[3:5] # Eliminamos la columna del sexo
datos$Compra <- factor(datos$Compra,
levels = c(0, 1),
labels = c(0, 1))
head(datos, 5)
# 3. Separar en Entrenamiento y Validación
set.seed(123)
split <- sample.split(datos$Compra, SplitRatio = 0.75)
train <- subset(datos, split==TRUE)
test  <- subset(datos, split==FALSE)
dim(train)/dim(test)
# 4. Hacer las prediciones para el conjunto de Validación
train[-3] <- scale(train[-3])
test[-3]  <- scale(test[-3])
# 5. Construir el Modelo
arbol <- rpart::rpart(formula = Compra ~ .,
data = train,
control = rpart.control(minsplit=1))
# 6. Hacer las prediciones para el conjunto de Validación
y_pred <- predict(arbol, newdata = test[-3], type = 'class')
# 7. Hacer la matriz de confusión
cm = table(test[, 3], y_pred)
cm
# 8. Echemos un vistazo a la pinta que tienen las predicciones
# 8.1. Conjunto de entrenamiento
set = train
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_pred <- predict(arbol, newdata = grid_set, type = 'class')
plot(set[, -3],
main = 'Regresión Logística (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# 8.2. Conjunto de validación
set = test
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_pred <- predict(arbol, newdata = grid_set, type = 'class')
plot(set[, -3],
main = 'Regresión Logística (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# 8. Echemos un vistazo a la pinta que tienen las predicciones
# 8.1. Conjunto de entrenamiento
set = train
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_grid <- predict(arbol, newdata = grid_set, type = 'class')
plot(set[, -3],
main = 'Regresión Logística (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# 8.2. Conjunto de validación
set = test
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_grid <- predict(arbol, newdata = grid_set, type = 'class')
plot(set[, -3],
main = 'Regresión Logística (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# 1. Importar librerías
library(caTools)
library(ggplot2)
library(rpart)         # Nos va a pertmit crear el arbol
library(ElemStatLearn) # Nos va a permitir dibujar las clasificaciones
# 2. Importar datos
datos <- read.csv('../Datos/4.2.Compras.csv')
datos <- datos[3:5] # Eliminamos la columna del sexo
datos$Compra <- factor(datos$Compra,
levels = c(0, 1),
labels = c(0, 1))
head(datos, 5)
# 3. Separar en Entrenamiento y Validación
set.seed(123)
split <- sample.split(datos$Compra, SplitRatio = 0.75)
train <- subset(datos, split==TRUE)
test  <- subset(datos, split==FALSE)
dim(train)/dim(test)
# 4. Hacer las prediciones para el conjunto de Validación
train[-3] <- scale(train[-3])
test[-3]  <- scale(test[-3])
# 5. Construir el Modelo
arbol <- rpart::rpart(formula = Compra ~ .,
data = train,
control = rpart.control(minsplit=1))
# 6. Hacer las prediciones para el conjunto de Validación
y_pred <- predict(arbol, newdata = test[-3], type = 'class')
# 7. Hacer la matriz de confusión
cm = table(test[, 3], y_pred)
cm
# 8. Echemos un vistazo a la pinta que tienen las predicciones
# 8.1. Conjunto de entrenamiento
set = train
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_grid <- predict(arbol, newdata = grid_set, type = 'class')
plot(set[, -3],
main = 'Árbol de Decision (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# 8.2. Conjunto de validación
set = test
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_grid <- predict(arbol, newdata = grid_set, type = 'class')
plot(set[, -3],
main = 'Árbol de Decision (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
bosque <- randomForest::randomForest(formula = Compra ~ .,
ntree = 50)
train <- subset(datos, split==TRUE)
test  <- subset(datos, split==FALSE)
train[-3] <- scale(train[-3])
test[-3]  <- scale(test[-3])
bosque <- randomForest::randomForest(formula = Compra ~ .,
ntree = 50)
bosque <- randomForest::randomForest(formula = Compra ~ .,
data = train
ntree = 50)
bosque <- randomForest::randomForest(formula = Compra ~ .,
data = train,
ntree = 50)
y_pred <- predict(arbol, newdata = test[-3], type = 'class')
cm = table(test[, 3], y_pred)
cm
# 1. Importar librerías
library(caTools)
library(ggplot2)
library(randomForest)  # Nos va a pertmit crear el bosque
library(ElemStatLearn) # Nos va a permitir dibujar las clasificaciones
# 2. Importar datos
datos <- read.csv('../Datos/4.2.Compras.csv')
datos <- datos[3:5] # Eliminamos la columna del sexo
datos$Compra <- factor(datos$Compra,
levels = c(0, 1),
labels = c(0, 1))
head(datos, 5)
# 3. Separar en Entrenamiento y Validación
set.seed(123)
split <- sample.split(datos$Compra, SplitRatio = 0.75)
train <- subset(datos, split==TRUE)
test  <- subset(datos, split==FALSE)
dim(train)/dim(test)
# 4. Hacer las prediciones para el conjunto de Validación
train[-3] <- scale(train[-3])
test[-3]  <- scale(test[-3])
# 5. Construir el Modelo
bosque <- randomForest::randomForest(formula = Compra ~ .,
data = train,
ntree = 50)
# 6. Hacer las prediciones para el conjunto de Validación
y_pred <- predict(arbol, newdata = test[-3], type = 'class')
# 7. Hacer la matriz de confusión
cm = table(test[, 3], y_pred)
cm
# 8. Echemos un vistazo a la pinta que tienen las predicciones
# 8.1. Conjunto de entrenamiento
set = train
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_grid <- predict(bosque, newdata = grid_set)
plot(set[, -3],
main = 'Bosque Aleatorio (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# 8.2. Conjunto de validación
set = test
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_grid <- predict(bosque, newdata = grid_set)
plot(set[, -3],
main = 'Bosque Aleatorio (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
y_pred <- predict(bosque, newdata = test[-3], type = 'class')
cm = table(test[, 3], y_pred)
cm
# 1. Importar librerías
library(caTools)
library(ggplot2)
library(randomForest)  # Nos va a pertmit crear el bosque
library(ElemStatLearn) # Nos va a permitir dibujar las clasificaciones
# 2. Importar datos
datos <- read.csv('../Datos/4.2.Compras.csv')
datos <- datos[3:5] # Eliminamos la columna del sexo
datos$Compra <- factor(datos$Compra,
levels = c(0, 1),
labels = c(0, 1))
head(datos, 5)
# 3. Separar en Entrenamiento y Validación
set.seed(123)
split <- sample.split(datos$Compra, SplitRatio = 0.75)
train <- subset(datos, split==TRUE)
test  <- subset(datos, split==FALSE)
dim(train)/dim(test)
# 4. Hacer las prediciones para el conjunto de Validación
train[-3] <- scale(train[-3])
test[-3]  <- scale(test[-3])
# 5. Construir el Modelo
bosque <- randomForest::randomForest(formula = Compra ~ .,
data = train,
ntree = 50)
# 6. Hacer las prediciones para el conjunto de Validación
y_pred <- predict(bosque, newdata = test[-3], type = 'class')
# 7. Hacer la matriz de confusión
cm = table(test[, 3], y_pred)
cm
# 8. Echemos un vistazo a la pinta que tienen las predicciones
# 8.1. Conjunto de entrenamiento
set = train
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_grid <- predict(bosque, newdata = grid_set)
plot(set[, -3],
main = 'Bosque Aleatorio (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# 8.2. Conjunto de validación
set = test
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Edad', 'Salario')
y_grid <- predict(bosque, newdata = grid_set)
plot(set[, -3],
main = 'Bosque Aleatorio (Conjunto de entrenamiento)',
xlab = 'Edad', ylab = 'Salario',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
